# 【GO】分布式缓存

```text
                            是
接收 key --> 检查是否被缓存 -----> 返回缓存值 ⑴
                |  否                         是
                |-----> 是否应当从远程节点获取 -----> 与远程节点交互 --> 返回缓存值 ⑵
                            |  否
                            |-----> 调用`回调函数`，获取值并添加到缓存 --> 返回缓存值 ⑶
```

## 缓存可能会遇到的问题

### 内存不够了怎么办？

通过删除部分数据来腾出内存，需要考虑删除那部分数据，随机删除、按时间顺序删除、根据访问频率删除等等，这就是选择淘汰策略。

### 并发写入冲突了怎么办？

应对并发场景，修改操作需要加锁（新增、更新和删除）

### 单机性能不够怎么办？

可通过水平扩展（scale horizontally）或者垂直扩展（scale vertically）

**水平扩展**即分布式系统

**垂直扩展**即增加单个节点的计算、存储、带宽等，来提高系统的性能，但是硬件成本和性能并非呈线性关系

## LRU 缓存淘汰策略

### LRU\(Least Recently Used\)

最近最少使用，相对于仅考虑时间因素的 FIFO 和仅考虑访问频率的 LFU，LRU 算法可以认为是相对平衡的一种淘汰算法。LRU 认为，如果数据最近被访问过，那么将来被访问的概率也会更高。LRU 算法的实现非常简单，维护一个队列，如果某条记录被访问了，则移动到队尾，那么队首则是最近最少访问的数据，淘汰该条记录即可。

![](../../.gitbook/assets/image%20%28133%29.png)

## 单机并发缓存

 多个协程\(goroutine\)同时读写同一个变量，在并发度较高的情况下，会发生冲突。确保一次只有一个协程\(goroutine\)可以访问该变量以避免冲突，这称之为`互斥`，互斥锁可以解决这个问题。

> sync.Mutex 是一个互斥锁，可以由不同的协程加锁和解锁。

 `sync.Mutex` 是 Go 语言标准库提供的一个互斥锁，当一个协程\(goroutine\)获得了这个锁的拥有权后，其它请求锁的协程\(goroutine\) 就会阻塞在 `Lock()` 方法的调用上，直到调用 `Unlock()` 锁被释放。

## HTTP服务端

 创建一个结构体 `HTTPPool`，作为承载节点间 HTTP 通信的核心数据结构（包括服务端和客户端）

## 一致性哈希

对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

## 分布式节点

 `HTTPPool` 实现了服务端功能，通信不仅需要服务端还需要客户端，因此，我们接下来要为 `HTTPPool` 实现客户端的功能。

## 防止缓存击穿

> **缓存雪崩**：缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。缓存雪崩通常因为缓存服务器宕机、缓存的 key 设置了相同的过期时间等引起。

> **缓存击穿**：一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到 DB ，造成瞬时DB请求量大、压力骤增。

> **缓存穿透**：查询一个不存在的数据，因为不存在则不会写到缓存中，所以每次都会去请求 DB，如果瞬间流量过大，穿透到 DB，导致宕机。

 我们并发了 N 个请求 `?key=Tom`，8003 节点向 8001 同时发起了 N 次请求。假设对数据库的访问没有做任何限制的，很可能向数据库也发起 N 次请求，容易导致缓存击穿和穿透。即使对数据库做了防护，HTTP 请求是非常耗费资源的操作，针对相同的 key，8003 节点向 8001 发起三次请求也是没有必要的。那这种情况下，我们如何做到只向远端节点发起一次请求呢？

## 使用Protobuf通信



* `ServeHTTP()` 中使用 `proto.Marshal()` 编码 HTTP 响应。
* `Get()` 中使用 `proto.Unmarshal()` 解码 HTTP 响应。

